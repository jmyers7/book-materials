\documentclass[12pt,reqno]{amsart}
\usepackage{./header, amssymb}

\hdr{Mathematical Statistics}{Chapter 9: Information theory}

\begin{document}

\bigskip

\prob Suppose $P$ is a probability measure defined on $S = \{1,2,3,4,5\}$ with mass function

	\[
	\begin{array}{c|c}
	s & p(s) \\ \hline
	1 & 0.1 \\
	2 & 0.3 \\
	3 & 0.2 \\
	4 & 0.3 \\
	5 & 0.1 
	\end{array}
	\]

Compute the information content $I(s)$ of each sample point and the entropy $H(P)$.

\bigskip
\textcolor{red}{We compute
	\[
	\begin{array}{c|cc}
	s & p(s) & I(s)\\ \hline
	1 & 0.1 & 2.303\\
	2 & 0.3 & 1.204\\
	3 & 0.2 & 1.609\\
	4 & 0.3 & 1.204\\
	5 & 0.1 & 2.303
	\end{array}
	\]
where the surprisals are rounded to three places after the decimal point. The entropy is $H(P) \approx 1.505$.}
\bigskip









\prob Let $X\sim \mathcal{B}er(\theta)$ for $\theta \in [0,1]$. Compute a formula for $H(X)$ in terms of $\theta$.

\bigskip
\textcolor{red}{We compute:
	\[H(X) = -\sum_{x=0}^1 p(x) \log(p(x)) = -(1-\theta) \log(1-\theta) - \theta \log(\theta).
	\]}
\bigskip









\prob Suppose $P$ and $Q$ are probability measures defined on $S = \{1,2,3,4,5\}$ with mass functions

	\[
	\begin{array}{c|cc}
	s & p(s) & q(s) \\ \hline
	1 & 0.1 & 0.05 \\
	2 & 0.3 & 0.15 \\
	3 & 0.2 & 0.7 \\
	4 & 0.3 & 0.03 \\
	5 & 0.1 & 0.07
	\end{array}
	\]

Compute the cross entropies $H_P( Q)$ and $H_Q( P)$.

\bigskip
\textcolor{red}{
	\[H_P(  Q) = - \sum_{s=1}^5 p(s) \log(q(s)) \approx 2.258, \quad H_Q( P) = -\sum_{s=1}^5 q(s) \log(p(s)) \approx 1.620.
	\]}
\bigskip








\prob Suppose $P$ and $Q$ are probability measures defined on $S = \{1,2,3,4,5\}$ with mass functions

	\[
	\begin{array}{c|cc}
	s & p(s) & q(s) \\ \hline
	1 & 0.1 & 0.05 \\
	2 & 0.3 & 0.15 \\
	3 & 0.2 & 0.7 \\
	4 & 0.3 & 0.03 \\
	5 & 0.1 & 0.07
	\end{array}
	\]

Compute $D( P \parallel Q)$ and $D(Q\parallel P)$.

\bigskip
\textcolor{red}{
	\[D( P \parallel Q) = \sum_{s=1}^5 p(s) \log \left( \frac{p(s)}{q(s)} \right) \approx 0.753 \quad \text{and} \quad D( Q \parallel P) = \sum_{s=1}^5 q(s) \log \left( \frac{q(s)}{p(s)} \right) \approx 0.644.
	\]}
\bigskip










\prob While it is \textbf{not} true that $D(P\parallel Q) = D(Q \parallel P)$ for \textbf{all} $P$ and $Q$, there are examples of special distributions $P$ and $Q$ for which equality does hold. Find examples.

\bigskip
\textcolor{red}{For $\theta \in [0,1]$, define $P_\theta$ and $Q_\theta$ on the sample space $S= \{0,1\}$ by
	\[\begin{array}{c|cc}
	s & p_\theta(s) & q_\theta(s) \\ \hline
	0 & \theta & 1-\theta \\
	1 & 1 - \theta & \theta
	\end{array}
	\]
One then easily proves $D(P_\theta \parallel Q_\theta) = D(Q_\theta \parallel P_\theta)$.}
\bigskip









\prob For each $\phi \in [0,1]$, consider the communication channel with transition matrix

	\[\mathbf{K} = \begin{bmatrix}
	1 - \phi & \phi \\
	\phi & 1-\phi
	\end{bmatrix}
	\]
	
These are called \textit{binary symmetric channels}. Suppose that $X \sim \mathcal{B}er(\alpha)$ for some $\alpha \in [0,1]$, with the range of $X$ enumerated as $x_0 = 0$ and $x_1=1$. Show that $X$ and the communication channel determine a random variable $Y$ with range $y_0=0$ and $y_1=1$. Determine its distribution.

\bigskip
\textcolor{red}{Since the range of $Y$ is $\{0,1\}$, it must be Bernoulli, with $Y \sim \mathcal{B}er(\beta)$ for some $\beta \in [0,1]$. We need to determine the parameter $\beta$. But notice that the probability vectors encoding the mass functions of $X$ and $Y$ have the form
	\[\boldsymbol{\pi}(X)^\intercal = \begin{bmatrix} 1- \alpha & \alpha \end{bmatrix} \quad \text{and} \quad \boldsymbol{\pi}(Y)^\intercal = \begin{bmatrix} 1- \beta & \beta \end{bmatrix}.
	\]
So, if we conceptualize the entries in the transition matrix as the conditional probabilities, then by the Law of Total Probability we must have
	\[\boldsymbol{\pi}(Y)^\intercal = \boldsymbol{\pi}(X)^\intercal\mathbf{K} = \begin{bmatrix} 1- \alpha & \alpha \end{bmatrix} \begin{bmatrix}
	1 - \phi & \phi \\
	\phi & 1-\phi
	\end{bmatrix}
 = \begin{bmatrix} (1-\alpha)(1-\phi) + \alpha \phi & (1-\alpha)\phi + \alpha(1-\phi) \end{bmatrix}.
	\]
Thus, $\beta = \phi + \alpha - 2\phi \alpha$.}
\bigskip








\prob Suppose $X$ and $Y$ are Bernoulli random variables with joint mass function given by
	\[\begin{array}{c|cc}
	p(x,y) & y=0 & y=1 \\ \hline
	x=0 & 0.3 & 0.1 \\ 
	x=1 & 0.36 & 0.24
	\end{array}
	\]
	
\bigskip
\begin{enumerate}
\item Compute the transition matrix of the communication channel induced from $X$ and $Y$.

\bigskip
\textcolor{red}{This just means that we need to compute the conditional mass function $p(y|x)$. We begin by computing the marginal mass $p(x)$:
	\[\begin{array}{c|c}
	x & p(x) \\ \hline
	0 & 0.4 \\
	1 & 0.6
	\end{array}
	\]
Then the transition matrix is given by
	\[\mathbf{K} = \left\{\begin{array}{c|cc}
	p(y|x) & y=0 & y=1 \\ \hline
	x = 0 & 0.75 & 0.25 \\
	x = 1 & 0.6 & 0.4
	\end{array}\right\}.
	\]}
\bigskip

\item Compute the mutual information $I(X,Y)$.

\bigskip
\textcolor{red}{$I(X,Y) \approx 0.012$}
\bigskip
\end{enumerate}


\end{document}