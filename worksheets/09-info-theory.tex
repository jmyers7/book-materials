\documentclass[12pt,reqno]{amsart}
\usepackage{./header, amssymb}

\hdr{Mathematical Statistics}{Chapter 9: Information theory}

\begin{document}

\bigskip

\prob Suppose $P$ is a probability measure defined on $S = \{1,2,3,4,5\}$ with mass function

	\[
	\begin{array}{c|c}
	s & p(s) \\ \hline
	1 & 0.1 \\
	2 & 0.3 \\
	3 & 0.2 \\
	4 & 0.3 \\
	5 & 0.1 
	\end{array}
	\]

Compute the information content $I(s)$ of each sample point and the entropy $H(P)$.










\vfill
\prob Let $X\sim \mathcal{B}er(\theta)$ for $\theta \in [0,1]$. Compute a formula for $H(X)$ in terms of $\theta$.










\vfill
\prob Suppose $P$ and $Q$ are probability measures defined on $S = \{1,2,3,4,5\}$ with mass functions

	\[
	\begin{array}{c|cc}
	s & p(s) & q(s) \\ \hline
	1 & 0.1 & 0.05 \\
	2 & 0.3 & 0.15 \\
	3 & 0.2 & 0.7 \\
	4 & 0.3 & 0.03 \\
	5 & 0.1 & 0.07
	\end{array}
	\]

Compute the cross entropies $H_P( Q)$ and $H_Q( P)$.











\vfill
\newpage
\prob Suppose $P$ and $Q$ are probability measures defined on $S = \{1,2,3,4,5\}$ with mass functions

	\[
	\begin{array}{c|cc}
	s & p(s) & q(s) \\ \hline
	1 & 0.1 & 0.05 \\
	2 & 0.3 & 0.15 \\
	3 & 0.2 & 0.7 \\
	4 & 0.3 & 0.03 \\
	5 & 0.1 & 0.07
	\end{array}
	\]

Compute $D( P \parallel Q)$ and $D(Q\parallel P)$.











\vfill
\prob While it is \textbf{not} true that $D(P\parallel Q) = D(Q \parallel P)$ for \textbf{all} $P$ and $Q$, there are examples of special distributions $P$ and $Q$ for which equality does hold. Find examples.










\vfill
\newpage
\prob For each $\phi \in [0,1]$, consider the communication channel with transition matrix

	\[\mathbf{K} = \begin{bmatrix}
	1 - \phi & \phi \\
	\phi & 1-\phi
	\end{bmatrix}
	\]
	
These are called \textit{binary symmetric channels}. Suppose that $X \sim \mathcal{B}er(\alpha)$ for some $\alpha \in [0,1]$, with the range of $X$ enumerated as $x_0 = 0$ and $x_1=1$. Show that $X$ and the communication channel determine a random variable $Y$ with range $y_0=0$ and $y_1=1$. Determine its distribution.









\vfill
\prob Suppose $X$ and $Y$ are Bernoulli random variables with joint mass function given by
	\[\begin{array}{c|cc}
	p(x,y) & y=0 & y=1 \\ \hline
	x=0 & 0.3 & 0.1 \\ 
	x=1 & 0.36 & 0.24
	\end{array}
	\]
	
\bigskip
\begin{enumerate}
\item Compute the transition matrix of the communication channel induced from $X$ and $Y$.


\vfill
\item Compute the mutual information $I(X,Y)$.


\end{enumerate}

\vfill
\end{document}