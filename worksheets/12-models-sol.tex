\documentclass[12pt,reqno]{amsart}
\usepackage{./header, amssymb}

\hdr{Mathematical Statistics}{Chapter 12: Probabilistic graphical models}

\begin{document}

\bigskip

\prob Suppose that we have three binary random variables $X$, $W$, and $H$, which indicate whether a person exercises regularly ($X$), whether they are overweight ($W$), and whether they have heart disease ($H$). Discuss several causal structure involving these variables.

\bigskip
\textcolor{red}{We first might believe that $X$ is a confounding variable in the relationship between $W$ and $H$, meaning the causal graph has the form $W \leftarrow X \rightarrow H$. This means a person's decision to exercise has a direct influence on both their weight and whether they have heart disease, and that any statistical correlation between the latter variables is the result of this common cause and not a result of a direct cause and effect relationship between the two. Alternatively, we might believe that $X$ serves as a mediating variable between $W$ and $H$, meaning the causal graph has the form $W \to X \to H$. This means that a person's weight directly influences their decision to exercise, and then this latter decision directly influences whether they have heart disease. However, this causal model still assumes there is not a direct cause and effect relationship between $W$ and $H$.}
\bigskip













\prob Let $X$ and $Y$ be binary random variables that indicate whether a person has \textit{hydromechanical trepidation syndrome} ($X$) and whether they test positive ($Y$) for it. Suppose that
	\[p(y=1 |x=1) = 0.98 \quad \text{and} \quad p(y=1 | x=0) = 0.1.
	\]
Express the flow of information from $X$ to $Y$ as a stochastic link by explicitly writing down a link function.

\bigskip
\textcolor{red}{The link from $X$ to $Y$ is given by
	\[Y \mid X \sim \mathcal{B}er(\theta), \quad \theta = g(x) = 0.1(1-x) + 0.98x.
	\]}
\bigskip
	







\prob Explicitly draw the full graphical structure for a plated linear regression model

\bigskip
\begin{center}
\includegraphics[scale=1.5]{lin-reg-00-plated.pdf}
\end{center}
\bigskip

when $m=3$.

\bigskip
\textcolor{red}{See your solutions from class.}
\bigskip













\prob Given a linear regression model with predictors $\mathbf{X}$, response $Y$, and parameters $\beta_0$, $\boldsymbol{\beta}$, and $\sigma^2$, we saw in class that
	\[ R \mid \mathbf{X} = \mathbf{x} \sim N(0, \sigma^2)
	\]
where $R = Y - \beta_0 - \mathbf{X}^\intercal \boldsymbol{\beta}$ is the error term. Prove this claim.

\bigskip
\textcolor{red}{Given $\mathbf{X} = \mathbf{x}$, we have
	\[R = Y - \beta_0 - \mathbf{x}^\intercal \boldsymbol{\beta},
	\]
where $Y \mid \mathbf{X} = \mathbf{x} \sim N(\mu,\sigma^2)$ with $\mu = \beta_0 + \mathbf{x}^\intercal \boldsymbol{\beta}$. This shows that $R \mid \mathbf{X} = \mathbf{x}$ is an affine transformation of a normal random variable, and so Theorem 5.7 applies to give us the desired result.}
\bigskip










\prob Compute the inverse of the sigmoid function $\sigma(x) = 1/(1+e^{-x})$. Interpret its meaning.

\bigskip
\textcolor{red}{Supposing $p = \sigma(x) = 1 / (1+e^{-x})$, easy algebra shows that the inverse function is given by
	\[x = \sigma^{-1}(p) = \log\left( \frac{p}{1-p} \right).
	\]
The ratio $p/(1-p)$ is often called the \textit{odds} of an event of probability $p$ occurring. If this event is drawn from a sample space with uniform probability measure, then it is simply the number of outcomes resulting in the event occurring, divided by the number of outcomes for which the event does not occur. Thus, the inverse function $\sigma^{-1}$ is often called the \textit{log-odds function}. (It is also sometimes called the \textit{logit function}.)}
\bigskip

\end{document}