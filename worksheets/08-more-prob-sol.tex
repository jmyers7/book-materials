\documentclass[12pt,reqno]{amsart}
\usepackage{./header, amssymb}

\hdr{Mathematical Statistics}{Chapter 8: More probability theory}

\begin{document}

\bigskip

\prob Suppose that $X$ and $Y$ are jointly continuous random variables with density

	\[
	f(x,y) = \begin{cases}
	24xy & : 0\leq x \leq 1, \ 0 \leq y \leq 1, \ x+y \leq 1, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]

Compute the expectation $E(XY)$.

\bigskip
\textcolor{red}{Using the bivariate LotUS, we get:
	\[E(XY) = \iint_{\mathbb{R}^2} xy f(x,y) \ \text{d}y\text{d}x = 24 \int_0^1 \int_0^{1-x} x^2y^2 \ \text{d}y\text{d}x = \frac{2}{15}.
	\]}
\bigskip














\prob Suppose $X$ and $Y$ are jointly continuous random variables with the same density from Problem 1. Compute a formula for the conditional expectation $E(Y\mid X=x)$. Take care to precisely state the domain of this function.

\bigskip
\textcolor{red}{We first get the marginal density for $X$:
	\[f(x) = \int_\mathbb{R} f(x,y) \ \text{d}y = 24\int_0^{1-x} xy \ \text{d}y = 12x(1-x)^2,
	\]
where the second inequality holds for all $0\leq x \leq 1$; otherwise, we have $f(x) =0$. Note that the marginal $f(x)$ is nonzero for $0<x<1$. For these particular $x$-values, the conditional density is defined and is given by
	\[
	f(y|x) = \frac{f(x,y)}{f(x)} = \begin{cases}
	\frac{2y}{(1-x)^2} & : 0\leq x \leq 1, \ 0 \leq y \leq 1, \ x+y \leq 1, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]
Therefore, for all $x$ with $0 < x< 1$, we have
	\[E(Y \mid X=x) = \int_\mathbb{R} y f(y|x) \ \text{d}y = \frac{2}{(1-x)^2} \int_0^{1-x} y^2 \ \text{d}y = \frac{2}{3}(1-x).
	\]}
\bigskip

















\prob Let $X$ and $Y$ be two random variables on the probability space $S = \{a,b,c\}$. Suppose that the probability distribution $P$ on $S$ has mass function $p(s)$ and that $X$ and $Y$ are defined according to the following table:

	\[
	\begin{array}{c|ccc}
	s & p(s) & X(s) & Y(s) \\ \hline
	a & 0.2 & 1 & 2 \\
	b & 0.5 & 2 & 1 \\
	c & 0.3 & 1 & 1
	\end{array}
	\] 

Compute the random variable $E(Y \mid X)$.

\bigskip
\textcolor{red}{Our first goal is to get the joint mass function $p(x,y)$. We compute:
	\[
	\begin{array}{c|cc}
	p(x,y) & y=1 & y=2 \\ \hline
	x=1 & 0.3 & 0.2 \\
	x=2 & 0.5 & 0
	\end{array}
	\]
By adding across the rows, we get the marginal:
	\[\begin{array}{c|c}
	x & p(x) \\ \hline
	1 & 0.5 \\
	2 & 0.5
	\end{array}
	\]
Then, from the formula $p(y|x) = p(x,y)/p(x)$, we get
	\[
	\begin{array}{c|cc}
	p(y|x) & y=1 & y=2 \\ \hline
	x=1 & 0.6 & 0.4 \\
	x=2 & 1 & 0
	\end{array}
	\]
If $x\neq 1$ or $2$, then the conditional mass $p(y|x)$ is undefined. For $x=1$ or $2$, we have
	\[E(Y \mid X=x) = \sum_{y=1}^2 y p(y|x) =  \begin{cases} 1 \cdot 0.6 + 2\cdot 0.4 = 1.4 & : x=1, \\
	1\cdot 1 + 2 \cdot 0 = 1 & : x=2.
	\end{cases}
	\]
Finally, recall that the random variable $E(Y\mid X)$ is defined as the composite $h(X) = h \circ X$, where $h(x) = E(Y \mid X=x)$ for $x=1,2$. Hence, we conclude the problem with:
	\[
	\begin{array}{c|c}
	s & E(Y\mid X)(s) \\ \hline
	a & 1.4 \\
	b & 1 \\
	c & 1.4
	\end{array}
	\] }
\bigskip















\prob Suppose that a point $X=x$ is chosen uniformly in the interval $(0,1)$. After $x$ has been chosen, suppose that a second point $Y=y$ is chosen uniformly in the interval $[x,1]$. Compute the expectation $E(Y)$.

\bigskip
\textcolor{red}{We will compute the expectation via the Law of Total Expectation:
	\[E(Y) = E \big[ E(Y\mid X) \big].
	\]
So, we begin by noting that $E(Y\mid X=x) = (x+1)/2 = x/2 + 1/2$, since the mean of a uniform distribution over an interval is the midpoint between its boundary points. Therefore $E(Y\mid X) = X/2 + 1/2$, and so
	\[E(Y) = E\big( X/2 + 1/2\big) = E(X) /2 + 1/2 = 1/4 + 1/2 = 3/4.
	\]}
\bigskip










\prob The waiting time $X$ in minutes between calls to a 911 center is exponentially distributed with mean $\mu = 2$ minutes. Compute the distribution of the transformed random variable $Y=60X$ that measures the waiting time in seconds.

\bigskip
\textcolor{red}{Since the mean of $X\sim \mathcal{E}xp(\lambda)$ is $1/\lambda$, we see that $\lambda = 1/2$ and so
	\[f_X(x) = \begin{cases}
	\frac{1}{2} e^{-x/2} & : x>0, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]
In the notation of the Density Transformation Theorem, the support of the density is $T = (0,\infty)$. If we define $r:T\to \mathbb{R}$ by $r(x) = 60x$, then the range $U$ of $r$ is the open interval $(0,\infty)$. Note that the inverse function $s:U\to \mathbb{R}$ is given by $s(y) = y/60$, which is continuously differentiable. Therefore, we have everything that we need to apply the Density Transformation Theorem. We get:
	\[f_Y(y) = \begin{cases}
	\frac{1}{120} e^{-y/120} & : y>0, \\
	0 & : \text{otherwise},
	\end{cases}
	\]
and thus $Y \sim \mathcal{E}xp(1/120)$.}
\bigskip












\prob Suppose that $X$ and $Y$ are two random variables such that $Y=e^X$ and $X\sim \mathcal{N}(\mu,\sigma^2)$. Compute the density of $Y$.

\bigskip
\textcolor{red}{The support of the density of $X$ is all of $\mathbb{R}$; so, in the notation of the Density Transformation Theorem we have $T = \mathbb{R}$. Define $r:T \to \mathbb{R}$ by setting $r(x) = e^x$, and note that the image $U$ of $r$ is the open interval $(0,\infty)$. The inverse function $s:U \to \mathbb{R}$ is given by $s(y) = \log{y}$, which is continuously differentiable. Therefore, we have everything that we need in order to use the Density Transformation Theorem. We compute:
	\[f_Y(y) = \begin{cases}
	\frac{1}{y\sqrt{2\pi \sigma^2}} \exp \left[ - \frac{1}{2\sigma^2} \left(\log{y} - \mu  \right)^2 \right] & : y>0, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]
Hence, $Y$ is a \textit{lognormal} random variable.}
\bigskip









\prob Suppose that $\mathbf{X}=(X_1,X_2)$ is a two-dimensional continuous random vector with density

	\[
	f(x_1,x_2) = \begin{cases}
	4x_1x_2 & : 0 < x_1 < 1, \ 0 < x_2 < 1, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]

Letting $T$ be the support of the density, define $r:T \to \mathbb{R}^2$ by setting

	\[r(x_1,x_2) = \left( \frac{x_1}{x_2}, x_1x_2 \right)
	\]
	
for $(x_1,x_2)\in \mathbb{R}^2$. Compute the density of the random vector $\mathbf{Y} = r(\mathbf{X})$.

\bigskip
\textcolor{red}{As I will explain during class, the image $U$ of $r$ is the open region in the $(y_1,y_2)$-plane bounded by the curves
	\[y_1 = y_2, \quad y_2 = 0, \quad y_2 = \frac{1}{y_2}.
	\]
To compute an inverse $s:U\to \mathbb{R}^2$ of $r$, notice that the equations
	\[y_1 = \frac{x_1}{x_2} \quad \text{and} \quad y_2 = x_1 x_2
	\]
can be solved uniquely for $x_1$ and $x_2$ yielding the solutions
	\[x_1 = \sqrt{y_1y_2} \quad \text{and} \quad x_2 = \sqrt{ \frac{y_2}{y_1}}.
	\]
Thus we may define $s$ by setting 
	\[s(y_1,y_2) = \left( \sqrt{y_1y_2}, \sqrt{ \frac{y_2}{y_1}}\right)
	\]
for all $(y_1,y_2) \in U$. To verify that $s$ is continuously differentiable, we compute its Jacobian matrix:
	\[\frac{\partial(s_1,s_2)}{\partial (y_1,y_2)}(y_1,y_2) = \begin{bmatrix}
	\frac{1}{2} \sqrt{\frac{y_2}{y_1}} & \frac{1}{2} \sqrt{\frac{y_1}{y_2}} \\
	-\frac{1}{2} \sqrt{ \frac{y_2}{y_1^3}} & \frac{1}{2} \sqrt{\frac{1}{y_1y_2}}
	\end{bmatrix}.
	\]
The four partial derivatives are all continuous on $U$, and thus $s$ is continuously differentiable. We therefore have everything that we need to apply the Density Transformation Theorem. First, we compute the determinant of the Jacobian matrix:
	\[\det \frac{\partial(s_1,s_2)}{\partial (y_1,y_2)}(y_1,y_2)  = \frac{1}{2y_1}.
	\]
Then, we have
	\[f\left( \sqrt{y_1y_2},\sqrt{\frac{y_2}{y_1}} \right)\cdot \frac{1}{2y_1} = 4\sqrt{y_1y_2}\cdot \sqrt{\frac{y_2}{y_1}}\cdot \frac{1}{2y_1} = \frac{2y_2}{y_1}
	\]
for $(y_1,y_2) \in U$, and thus
	\[f(y_1,y_2) = \begin{cases}
	\frac{2y_2}{y_1} & : (y_1,y_2) \in U, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]}
\bigskip



\prob Suppose that $X$ is a continuous random variable with uniform distribution on $[a,b]$. Compute its moment generating function $\psi(t)$, and then find all moments $E(X^k)$, for $k\geq 1$.

\bigskip
\textcolor{red}{We have
	\[f(x) = \frac{1}{b-a}
	\]
for all $x\in [a,b]$ and $f(x) = 0$ otherwise. Thus, by the LotUS, we have
	\[\psi(t) = E(e^{tX}) = \frac{1}{b-a} \int_a^b e^{tx} \ \text{d}x = \frac{e^{tb} - e^{ta}}{t(b-a)}.
	\]
If we expand the exponentials, we get
	\begin{align*}
	\psi(t) &= \frac{1}{t(b-a)} \left[ \sum_{k=0}^\infty \frac{t^kb^k}{k!} - \sum_{k=0}^\infty \frac{t^ka^k}{k!} \right] \\
	&= \frac{1}{b-a} \sum_{k=0}^\infty \frac{b^{k+1} - a^{k+1}}{(k+1)!} t^k
	\end{align*}
for all $t\in \mathbb{R}$. Thus,
	\[E(X^k) = \psi^{(k)}(0) = \frac{b^{k+1} - a^{k+1}}{(k+1)(b-a)} = \frac{a^k + a^{k-1}b + \cdots + ab^{k-1} + b^k}{k+1}.
	\]}
\bigskip











\prob Use moment generating functions to confirm that the mean and variance of a random variable $X \sim \mathcal{N}(\mu,\sigma^2)$ are indeed $\mu$ and $\sigma^2$.

\bigskip
\textcolor{red}{Letting $\psi(t)$ be the moment generating function of $X$, we have
	\[\psi'(t) = \psi(t) (\mu + \sigma^2 t),
	\]
and so
	\[E(X) = \psi'(0) =  \mu.
	\]
To compute the variance, we first compute
	\[\psi''(t) = \psi(t) (\mu + \sigma^2 t)^2 + \psi(t) \sigma^2.
	\]
Then
	\[E(X^2) = \psi''(0) = \mu^2 + \sigma^2,
	\]
and so
	\[V(X) = E(X^2) - E(X)^2 = \mu^2 + \sigma^2 - \mu^2 = \sigma^2.
	\]}
\bigskip












\prob Suppose that $X$ and $Y$ are random variables with the joint density function

	\[
	f(x,y) = \begin{cases}
	2xy + 0.5 & : 0 \leq x\leq 1, \ 0 \leq y \leq 1, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]
	
Compute the covariance of $X$ and $Y$.

\bigskip
\textcolor{red}{Using the Shortcut Formula for Covariance, we compute:
	\[
	\sigma_{XY} = E(XY) - E(X) E(Y).
	\]
But first, let's grab the expectations of $X$ and $Y$. To do this, we integrate out $y$ to get the density of $x$:
	\[f(x) = \int_{\mathbb{R}} f(x,y) \ \text{d} y = \int_0^1 \left( 2xy+0.5\right) \ \text{d}y = x + 0.5
	\]
for $0\leq x\leq 1$, and $f(x)=0$ otherwise. Then, we compute:
	\[E(X) = \int_{\mathbb{R}} x f(x) \ \text{d}x = \int_0^1 (x^2+0.5x) \ \text{d} x = \frac{7}{12}.
	\]
Now, if you look at the joint density function, you'll notice that it is symmetric in $x$ and $y$. This means that $E(Y) = 7/12$, as well. Finally, we compute the covariance from the shortcut formula:
	\begin{align*}
	\sigma_{XY} &= E(XY) - \frac{7^2}{12^2} \\
	&= \iint_{\mathbb{R}^2} xy f(x,y) \ \text{d}y \text{d}x - \frac{7^2}{12^2} \\
	&= \int_0^1 \int_0^1 (2x^2y^2 + 0.5xy) \ \text{d}y \text{d}x - \frac{7^2}{12^2} \\
	&= \frac{25}{72} - \frac{7^2}{12^2} \\
	&= \frac{1}{144} \\
	&\approx 0.007.
	\end{align*}}
\bigskip








\prob Compute the correlation $\rho_{XY}$ of the random variables in the previous problem.

\bigskip
\textcolor{red}{First, we compute the variances:
	\[\sigma_X^2 = \int_\mathbb{R} x^2 f(x) \ \text{d}x = \int_0^1 x^2(x+0.5) \ \text{d} x =  \frac{5}{12}.
	\]
By symmetry, we also have $\sigma_Y^2 = 5/12$. Thus,
	\[\rho_{XY} = \frac{\sigma_{XY}}{\sigma_X\sigma_Y} = \frac{1/144}{(5/12)^2} = \frac{1}{60} \approx 0.017.
	\]}
\bigskip









\prob Many students applying for college take the SAT, which consists of math and verbal components (the latter is currently called evidence-based reading and writing). Let $X$ and $Y$ denote the math and verbal scores, respectively, for a randomly selected student. According to the College Board, the population of students taking the exam in 2017 had the following results:
	\[\mu_X = 527, \quad \sigma_X = 107, \quad \mu_Y = 533, \quad \sigma_Y = 100, \quad \rho_{XY} = 0.77.
	\]

Supposing that $(X,Y) \sim \mathcal{N}_2(\boldsymbol\mu,\boldsymbol\Sigma)$, determine the probability that a student's total score $X+Y$ exceeds 1250, the minimum admission score for a particular university.

\bigskip
\textcolor{red}{From class, we learned that the random variable $Z =X+Y$ is normal. Using linearity of expectation, we compute its mean
	\[E(Z) = \mu_X + \mu_Y = 527 + 533 = 1060
	\]
and using bilinearity and symmetry of covariance we compute its variance
	\begin{align*}
	\sigma_Z^2 &= \operatorname{Cov}(X+Y,X+Y)  \\
	&= \operatorname{Cov}(X,X) + \operatorname{Cov}(Y,Y) + 2\operatorname{Cov}(X,Y) \\
	&= \sigma_X^2 + \sigma_Y^2 + 2 \rho_{XY} \sigma_{X}\sigma_Y \\
	&= 107 ^ 2 + 100 ^ 2 + 2(0.77)(107)(100) \\
	&= 37{,}927.
	\end{align*}
Thus, $Z \sim \mathcal{N}(1060, 37{,}927)$. Using technology, we then compute
	\[P(X+Y > 1250) = P(Z > 1250) = 1 - F_Z(1250) \approx 0.165.
	\]}


\end{document}