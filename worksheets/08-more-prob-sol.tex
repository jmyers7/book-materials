\documentclass[12pt,reqno]{amsart}
\usepackage{./header, amssymb}

\hdr{Mathematical Statistics}{Chapter 8: More probability theory}

\begin{document}

\bigskip

\prob Suppose that $X$ and $Y$ are jointly continuous random variables with density

	\[
	f(x,y) = \begin{cases}
	24xy & : 0\leq x \leq 1, \ 0 \leq y \leq 1, \ x+y \leq 1, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]

Compute the expectation $E(XY)$.

\bigskip
\textcolor{red}{Using the bivariate LoTUS, we get:
	\[E(XY) = \iint_{\mathbb{R}^2} xy f(x,y) \ \text{d}y\text{d}x = 24 \int_0^1 \int_0^{1-x} x^2y^2 \ \text{d}y\text{d}x = \frac{2}{15}.
	\]}
\bigskip














\prob Suppose $X$ and $Y$ are jointly continuous random variables with the same density from Problem 1. Compute a formula for the conditional expectation $E(Y\mid X=x)$. Take care to precisely state the domain of this function.

\bigskip
\textcolor{red}{We first get the marginal density for $X$:
	\[f(x) = \int_\mathbb{R} f(x,y) \ \text{d}y = 24\int_0^{1-x} xy \ \text{d}y = 12x(1-x)^2,
	\]
where the second inequality holds for all $0\leq x \leq 1$; otherwise, we have $f(x) =0$. Note that the marginal $f(x)$ is nonzero for $0<x<1$. For these particular $x$-values, the conditional density is defined and is given by
	\[
	f(y|x) = \frac{f(x,y)}{f(x)} = \begin{cases}
	\frac{2y}{(1-x)^2} & : 0\leq x \leq 1, \ 0 \leq y \leq 1, \ x+y \leq 1, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]
Therefore, for all $x$ with $0 < x< 1$, we have
	\[E(Y \mid X=x) = \int_\mathbb{R} y f(y|x) \ \text{d}y = \frac{2}{(1-x)^2} \int_0^{1-x} y^2 \ \text{d}y = \frac{2}{3}(1-x).
	\]}
\bigskip

















\prob Let $X$ and $Y$ be two random variables on the probability space $S = \{a,b,c\}$. Suppose that the probability distribution $P$ on $S$ has mass function $p(s)$ and that $X$ and $Y$ are defined according to the following table:

	\[
	\begin{array}{c|ccc}
	s & p(s) & X(s) & Y(s) \\ \hline
	a & 0.2 & 1 & 2 \\
	b & 0.5 & 2 & 1 \\
	c & 0.3 & 1 & 1
	\end{array}
	\] 

Compute the random variable $E(Y \mid X)$.

\bigskip
\textcolor{red}{Our first goal is to get the joint mass function $p(x,y)$. We compute:
	\[
	\begin{array}{c|cc}
	p(x,y) & y=1 & y=2 \\ \hline
	x=1 & 0.3 & 0.2 \\
	x=2 & 0.5 & 0
	\end{array}
	\]
By adding across the rows, we get the marginal:
	\[\begin{array}{c|c}
	x & p(x) \\ \hline
	1 & 0.5 \\
	2 & 0.5
	\end{array}
	\]
Then, from the formula $p(y|x) = p(x,y)/p(x)$, we get
	\[
	\begin{array}{c|cc}
	p(y|x) & y=1 & y=2 \\ \hline
	x=1 & 0.6 & 0.4 \\
	x=2 & 1 & 0
	\end{array}
	\]
If $x\neq 1$ or $2$, then the conditional mass $p(y|x)$ is undefined. For $x=1$ or $2$, we have
	\[E(Y \mid X=x) = \sum_{y=1}^2 y p(y|x) =  \begin{cases} 1 \cdot 0.6 + 2\cdot 0.4 = 1.4 & : x=1, \\
	1\cdot 1 + 2 \cdot 0 = 1 & : x=2.
	\end{cases}
	\]
Finally, recall that the random variable $E(Y\mid X)$ is defined as the composite $h(X) = h \circ X$, where $h(x) = E(Y \mid X=x)$ for $x=1,2$. Hence, we conclude the problem with:
	\[
	\begin{array}{c|c}
	s & E(Y\mid X)(s) \\ \hline
	a & 1.4 \\
	b & 1 \\
	c & 1.4
	\end{array}
	\] }
\bigskip















\prob Suppose that a point $X=x$ is chosen uniformly in the interval $(0,1)$. After $x$ has been chosen, suppose that a second point $Y=y$ is chosen uniformly in the interval $[x,1]$. Compute the expectation $E(Y)$.

\bigskip
\textcolor{red}{We will compute the expectation via the Law of Total Expectation:
	\[E(Y) = E \big[ E(Y\mid X) \big].
	\]
So, we begin by noting that $E(Y\mid X=x) = (x+1)/2 = x/2 + 1/2$, since the mean of a uniform distribution over an interval is the midpoint between its boundary points. Therefore $E(Y\mid X) = X/2 + 1/2$, and so
	\[E(Y) = E\big( X/2 + 1/2\big) = E(X) /2 + 1/2 = 1/4 + 1/2 = 3/4.
	\]}
\bigskip










\prob The waiting time $X$ in minutes between calls to a 911 center is exponentially distributed with mean $\mu = 2$ minutes. Compute the distribution of the transformed random variable $Y=60X$ that measures the waiting time in seconds.

\bigskip
\textcolor{red}{Since the mean of $X\sim \mathcal{E}xp(\lambda)$ is $1/\lambda$, we see that $\lambda = 1/2$ and so
	\[f_X(x) = \frac{1}{2} e^{-x/2}
	\]
for all $x>0$. Using the Density Transformation Theorem with $r(x) = 60x$ and $s(y) = x/60$, we have
	\[f_Y(y) = \frac{1}{2} e^{-y/120} \cdot \frac{1}{60} = \frac{1}{120} e^{-y/120}
	\]
for all $y>0$ and $f_Y(y)=0$ otherwise. Thus, $Y \sim \mathcal{E}xp(1/120)$.}
\bigskip












\prob Suppose that $X$ and $Y$ are two random variables such that $Y=e^X$ and $X\sim \mathcal{N}(\mu,\sigma^2)$. Compute the density of $Y$.

\bigskip
\textcolor{red}{We use the Density Transformation Theorem with $r(x) = e^x$ and $s(y) = \log{y}$:
	\[f_Y(y) = f_X(\log{y}) \frac{1}{y} = \frac{1}{y\sqrt{2\pi \sigma^2}} \exp \left[ - \frac{1}{2\sigma^2} \left(\log{y} - \mu  \right)^2 \right],
	\]
for all $y>0$ and $f_Y(y)=0$ otherwise.}
\bigskip









\prob Suppose that $\mathbf{X}=(X_1,X_2)$ is a two-dimensional continuous random vector with density

	\[
	f(x_1,x_2) = \begin{cases}
	4x_1x_2 & : 0 < x_1 < 1, \ 0 < x_2 < 1, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]

For all $(x_1,x_2) \in \mathbb{R}^2$ with $x_2 \neq 0$, define

	\[(y_1,y_2) = r(x_1,x_2) = \left( \frac{x_1}{x_2}, x_1x_2 \right).
	\]
	
Compute the density of the random vector $\mathbf{Y} = r(\mathbf{X})$.

\bigskip
\textcolor{red}{Notice that this is a case where the function $r$ is not defined on \textit{all} of $\mathbb{R}^2$, but rather only on an open subset of $\mathbb{R}^2$ that contains the support of the density $f(x_1,x_2)$. But, as I mentioned, the conclusions of the Density Transformation Theorem are not altered. So, we proceed with applying the theorem, restricting $r$ to the support of $f(x,y)$. Let's first compute the Jacobian matrix of $r$:
	\[\frac{\partial(r_1,r_2)}{\partial(x_1,x_2)} = \begin{bmatrix} \frac{1}{x_2} & - \frac{x_1}{x_2^2} \\ x_2 & x_1 \end{bmatrix}.
	\]
Since the partial derivatives exist and are continuous at all points in the support of the density, we conclude that $r$ is continuously differentiable. Moreover, the determinant of the Jacobian matrix is easily computed to be $2x_1/x_2$, which does not vanish at any point in the support. The last hypothesis that we need to check regarding $f$ is its injectivity; but notice that the equations
	\[y_1 = \frac{x_1}{x_2} \quad \text{and} \quad y_2 = x_1 x_2
	\]
can be solved uniquely for $x_1$ and $x_2$ yielding the solutions
	\[x_1 = \sqrt{y_1y_2} \quad \text{and} \quad x_2 = \sqrt{ \frac{y_2}{y_1}}.
	\]
This shows $r$ is one-to-one and gives the formula for the inverse function $s$ defined on the range $A$ of $r$:
	\[(x_1,x_2) = s(y_1,y_2) = \left( \sqrt{y_1y_2}, \sqrt{ \frac{y_2}{y_1}}\right).
	\]
Thus, all the hypotheses for $r$ in the statement of the theorem are true. As I will explain during class, the image $A$ of $r$ is the unbounded, open region in the $(y_1,y_2)$-plane bounded by the curves
	\[y_1 = y_2, \quad y_2 = 0, \quad y_2 = \frac{1}{y_2}.
	\]
For those points $(y_1,y_2)$ in this latter region, we compute the determinant of the Jacobian matrix:
	\[\det\frac{\partial(s_1,s_2)}{\partial (y_1,y_2)} = \det\begin{bmatrix}
	\frac{1}{2} \sqrt{\frac{y_2}{y_1}} & \frac{1}{2} \sqrt{\frac{y_1}{y_2}} \\
	-\frac{1}{2} \sqrt{ \frac{y_2}{y_1^3}} & \frac{1}{2} \sqrt{\frac{1}{y_1y_2}}
	\end{bmatrix} = \frac{1}{2y_1}.
	\]
Thus, the desired density is defined for all $(y_1,y_2)$ in the region described above by the formula
	\[f(y_1,y_2) = f\left( \sqrt{y_1y_2},\sqrt{\frac{y_2}{y_1}} \right) = 4\sqrt{y_1y_2}\cdot \sqrt{\frac{y_2}{y_1}}\cdot \frac{1}{2y_1} = \frac{2y_2}{y_1},
	\]
while $f(y_1,y_2) =0$ for all $(y_1,y_2)$ outside this region.}
\bigskip



\prob Suppose that $X$ is a continuous random variable with uniform distribution on $[a,b]$. Compute its moment generating function $\psi(t)$, and then find all moments $E(X^k)$, for $k\geq 1$.

\bigskip
\textcolor{red}{We have
	\[f(x) = \frac{1}{b-a}
	\]
for all $x\in [a,b]$ and $f(x) = 0$ otherwise. Thus, by the LoTUS, we have
	\[\psi(t) = E(e^{tX}) = \frac{1}{b-a} \int_a^b e^{tx} \ \text{d}x = \frac{e^{tb} - e^{ta}}{t(b-a)}.
	\]
If we expand the exponentials, we get
	\begin{align*}
	\psi(t) &= \frac{1}{t(b-a)} \left[ \sum_{k=0}^\infty \frac{t^kb^k}{k!} - \sum_{k=0}^\infty \frac{t^ka^k}{k!} \right] \\
	&= \frac{1}{b-a} \sum_{k=0}^\infty \frac{b^{k+1} - a^{k+1}}{(k+1)!} t^k
	\end{align*}
for all $t\in \mathbb{R}$. Thus,
	\[E(X^k) = \psi^{(k)}(0) = \frac{b^{k+1} - a^{k+1}}{(k+1)(b-a)} = \frac{a^k + a^{k-1}b + \cdots + ab^{k-1} + b^k}{k+1}.
	\]}
\bigskip











\prob Use moment generating functions to confirm that the mean and variance of a random variable $X \sim \mathcal{N}(\mu,\sigma^2)$ are indeed $\mu$ and $\sigma^2$.

\bigskip
\textcolor{red}{Letting $\psi(t)$ be the moment generating function of $X$, we have
	\[\psi'(t) = \psi(t) (\mu + \sigma^2 t),
	\]
and so
	\[E(X) = \psi'(0) =  \mu.
	\]
To compute the variance, we first compute
	\[\psi''(t) = \psi(t) (\mu + \sigma^2 t)^2 + \psi(t) \sigma^2.
	\]
Then
	\[E(X^2) = \psi''(0) = \mu^2 + \sigma^2,
	\]
and so
	\[V(X) = E(X^2) - E(X)^2 = \mu^2 + \sigma^2 - \mu^2 = \sigma^2.
	\]}
\bigskip












\prob Suppose that $X$ and $Y$ are random variables with the joint density function

	\[
	f(x,y) = \begin{cases}
	2xy + 0.5 & : 0 \leq x, y \leq 1, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]
	
Compute the covariance of $X$ and $Y$.

\bigskip
\textcolor{red}{Using the Shortcut Formula for Covariance, we compute:
	\[
	\sigma_{XY} = E(XY) - E(X) E(Y).
	\]
But first, let's grab the expectations of $X$ and $Y$. To do this, we integrate out $y$ to get the density of $x$:
	\[f(x) = \int_{\mathbb{R}} f(x,y) \ \text{d} y = \int_0^1 \left( 2xy+0.5\right) \ \text{d}y = x + 0.5
	\]
for $0\leq x\leq 1$, and $f(x)=0$ otherwise. Then, we compute:
	\[E(X) = \int_{\mathbb{R}} x f(x) \ \text{d}x = \int_0^1 (x^2+0.5x) \ \text{d} x = \frac{7}{12}.
	\]
Now, if you look at the joint density function, you'll notice that it is symmetric in $x$ and $y$. This means that $E(Y) = 7/12$, as well. Finally, we compute the covariance from the shortcut formula:
	\begin{align*}
	\sigma_{XY} &= E(XY) - \frac{7^2}{12^2} \\
	&= \iint_{\mathbb{R}^2} xy f(x,y) \ \text{d}y \text{d}x - \frac{7^2}{12^2} \\
	&= \int_0^1 \int_0^1 (2x^2y^2 + 0.5xy) \ \text{d}y \text{d}x - \frac{7^2}{12^2} \\
	&= \frac{25}{72} - \frac{7^2}{12^2} \\
	&= \frac{1}{144} \\
	&\approx 0.007.
	\end{align*}}













\bigskip
\prob Suppose that $X$ and $Y$ are random variables with the joint density function

	\[
	f(x,y) = \begin{cases}
	3x & : 0 \leq y\leq x \leq 1, \\
	0 & : \text{otherwise}.
	\end{cases}
	\]
	
Compute the covariance of $X$ and $Y$.


\bigskip
\textcolor{red}{We follow the same strategy as the previous problem. First, we get the marginal densities:
	\[f(x) = \int_{\mathbb{R}} f(x,y) \ \text{d}y = \int_0^x 3x \ \text{d}y = 3x^2
	\]
for $0\leq x \leq 1$ and $f(x)=0$ otherwise; also:
	\[f(y) = \int_{\mathbb{R}} f(x,y) \ \text{d} x = \int_y^1 3x \ \text{d}x = \frac{3}{2}(1-y^2)
	\]
for $0\leq y \leq 1$. Then, we compute:
	\[E(X) = \int_{\mathbb{R}} x f(x) \ \text{d}x = \int_0^1 3x^3 \ \text{d}x = \frac{3}{4}
	\]
and
	\[E(Y) = \int_{\mathbb{R}} y f(y) \ \text{d}y = \frac{3}{2} \int_0^1 y (1-y^2) \ \text{d}y = \frac{3}{8}.
	\]
Finally, we compute
	\[E(XY) = \iint_{\mathbb{R}^2} xy f(x,y) \ \text{d}y\text{d}x = \int_0^1 \int_0^x 3x^2y \ \text{d}y\text{d}x = \frac{3}{10}
	\]
and hence
	\[\sigma_{XY} = E(XY) - E(X) E(Y) = \frac{3}{10} - \frac{3}{4} \cdot \frac{3}{8} = \frac{3}{160} \approx 0.019.
	\]}
	


\end{document}