{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mShkhXQolYkO"
      },
      "source": [
        "# Chapter 12: Probabilistic modeling with scikit-learn\n",
        "\n",
        "Through the [current chapter](https://mml.johnmyersmath.com/stats-book/chapters/12-models.html) of the book, we have studied several examples of _probabilistic graphical models_ (*PGM*s). However, we must wait until the [next chapter](https://mml.johnmyersmath.com/stats-book/chapters/13-learning.html) before we are able to train the models from scratch.\n",
        "\n",
        "But fortunately for us, the [scikit-learn](https://scikit-learn.org/stable/index.html) library in Python contains many implementations of probabilistic models that we may use _without_ knowing the underlying details on the training and fitting processes. The purpose of this programming assignment is to introduce scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kESJJFR1lijd"
      },
      "source": [
        "## Directions\n",
        "\n",
        "1. The programming assignment is organized into sequences of short problems. You can see the structure of the programming assignment by opening the \"Table of Contents\" along the left side of the notebook (if you are using Google Colab or Jupyter Lab).\n",
        "\n",
        "2. Do not add any cells of your own to the notebook, or delete any existing cells (either code or markdown)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiLQw9AKlkdb"
      },
      "source": [
        "## Submission instructions\n",
        "\n",
        "1. Once you have finished entering all your solutions, you will want to rerun all cells from scratch to ensure that everything works OK. To do this in Google Colab, click \"Runtime -> Restart and run all\" along the top of the notebook.\n",
        "\n",
        "2. Now scroll back through your notebook and make sure that all code cells ran properly.\n",
        "\n",
        "3. If everything looks OK, save your assignment and upload the `.ipynb` file at the provided link on the course <a href=\"https://github.com/jmyers7/stats-book-materials\">GitHub repo</a>. Late submissions are not accepted.\n",
        "\n",
        "4. You may submit multiple times, but I will only grade your last submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B0der20a0gO"
      },
      "source": [
        "## Polynomial regression models\n",
        "\n",
        "The first types of PGMs that we shall consider are straightforward generalizations of the linear regression models we studied [in the book](https://mml.johnmyersmath.com/stats-book/chapters/12-models.html#linear-regression-models). They are called _polynomial regression models_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THxV8c7ozf5j"
      },
      "source": [
        "### Training and fitting\n",
        "\n",
        "A _single-variable polynomial regression model of degree $d$_ is a probabilistic graphical model with underlying graph\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/img/poly-reg.svg\" width=\"300\" align=\"center\">\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "where $X$ and $Y$ are random variables. The parameters are:\n",
        "\n",
        "* a real number $\\beta_0 \\in \\mathbb{R}$,\n",
        "* a vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^d$,\n",
        "* a positive real number $\\sigma^2 >0$.\n",
        "\n",
        "The link function at $Y$ is of the form\n",
        "\n",
        "$$\n",
        "Y \\mid X = x; \\beta_0,\\boldsymbol{\\beta},\\sigma^2 \\sim N(\\mu,\\sigma^2) \\quad \\text{where} \\quad \\mu = \\beta_0 +\\beta_1x + \\cdots + \\beta_dx^d,\n",
        "$$\n",
        "\n",
        "and where $\\boldsymbol{\\beta}^\\intercal = (\\beta_1,\\beta_2,\\ldots,\\beta_d)$.\n",
        "\n",
        "As you might imagine, a (single-variable) polynomial regression model is appropriate for datasets\n",
        "\n",
        "$$\n",
        "(x_1,y_1),(x_2,y_2),\\ldots,(x_m,y_m) \\in \\mathbb{R}^2\n",
        "$$\n",
        "\n",
        "where we believe that\n",
        "\n",
        "$$\n",
        "y_i \\approx \\beta_0 + \\beta_1 x + \\cdots + \\beta_d x^d\n",
        "$$\n",
        "\n",
        "for each $i=1,\\ldots,m$, where $\\beta_0,\\beta_1,\\ldots,\\beta_d$ are fixed parameters.\n",
        "\n",
        "Let's look at an example. Run the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPh0LzO5cXzA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install math_stats_ml>=0.0.14\n",
        "from math_stats_ml.autograders.assignment_12 import *\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/data/data-12-1.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "X = df['x'].to_numpy().reshape(-1, 1)\n",
        "y = df['y'].to_numpy()\n",
        "m = len(X)\n",
        "\n",
        "plt.scatter(X, y, s=20)\n",
        "plt.ylim(-2, 2)\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$y$')\n",
        "plt.gcf().set_size_inches(w=5, h=4)\n",
        "plt.title(f'$m={m}$ data points')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYwQzvCGraes"
      },
      "source": [
        "The data looks like it roughly falls along the graph of a polynomial, right?\n",
        "\n",
        "Let's use scikit-learn to fit a polynomial regression model of degree $3$. We will accomplish this by first creating polynomial features from the original $x$-values. Specifically, suppose that the $x$-values in our original dataset make up the column of an $m\\times 1$ matrix (i.e., a column vector) called the [_design matrix_](https://en.wikipedia.org/wiki/Design_matrix):\n",
        "\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{m} \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "scikit-learn contains a convenient utility to create \"polynomial features\" (of degree $d$) from a given design matrix $\\mathbf{X}$, producing a new design matrix of the form\n",
        "\n",
        "$$\n",
        "\\mathbf{X}_\\text{poly} = \\begin{bmatrix}\n",
        "x_1 & x_1^2 & \\cdots & x_1^d \\\\\n",
        "x_2 & x_2^2 & \\cdots & x_2^d \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_m & x_m^2 & \\cdots & x_m^d\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "If you look in the code cell above, you'll notice that I already implemented the original design matrix as a NumPy array `X` of shape `(32, 1)`. Run the next cell to check it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNLn3_HPtC6K"
      },
      "outputs": [],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHpTtzYBtHZ6"
      },
      "source": [
        "To create polynomial features of degree $3$ for our polynomial regression model, we import [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) from the `preprocessing` submodule of scikit-learn, fit it to the original design matrix, and then transform the original design matrix to create `X_poly`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KJGhdottMkZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Instantiate a PolynomialFeatures object. Set the degree to 3, and\n",
        "# exclude the bias term from the polynomial features by setting the\n",
        "# `include_bias` parameter to `False`.\n",
        "pf = PolynomialFeatures(degree=3, include_bias=False)\n",
        "\n",
        "# Fit to the original design matrix, then transform it.\n",
        "X_poly = pf.fit_transform(X)\n",
        "\n",
        "# Display the polynomial features.\n",
        "X_poly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cCXraNGtwG6"
      },
      "source": [
        "The array `X_poly` is a NumPy array of shape `(32, 3)`. You can check using a calculator that the rows really are of the form $x, x^2, x^3$.\n",
        "\n",
        "Now, all we need to do to fit (i.e., train) a degree-$3$ polynomial regression model is to fit a **linear** regression model on the polynomial features in `X_poly`. This is how we do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHObEbG1uZSr"
      },
      "outputs": [],
      "source": [
        "# Import `LinearRegression` from the `linear_model` submodule.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Instantiate a `LinearRegression` object with default parameters.\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model to the polynomial features. The array `y` contains the original\n",
        "# y-values in the dataset.\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Get the learned coefficients through the `intercept_` and `coef_` attributes of\n",
        "# the model.\n",
        "beta0, beta = model.intercept_, model.coef_\n",
        "\n",
        "# Print the coefficients.\n",
        "print(f'beta_0 = {beta0:0.4f}\\nbeta_1 = {beta[0]:0.4f}\\nbeta_2 = {beta[1]:0.4f}\\nbeta_3 = {beta[2]:0.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR7G5Hj-wbUu"
      },
      "source": [
        "### Visualizing goodness of fit\n",
        "\n",
        "We want to visualize the goodness of fit of the model! Using the learned coefficients, here's how we would do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W279Ds1XwlwM"
      },
      "outputs": [],
      "source": [
        "# Define polynomial regression function using the learned coefficients.\n",
        "def f3(x, beta0, beta):\n",
        "  beta1 = beta[0]\n",
        "  beta2 = beta[1]\n",
        "  beta3 = beta[2]\n",
        "  return beta0 + beta1 * x + beta2 * x ** 2 + beta3 * x ** 3\n",
        "\n",
        "# Plot the data and the cubic polynomial.\n",
        "grid = np.linspace(-0.5, 2.5, 200)\n",
        "plt.scatter(X, y, s=20)\n",
        "plt.plot(grid, f3(grid, beta0, beta), color='orange')\n",
        "plt.ylim(-2, 2)\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$y$')\n",
        "plt.gcf().set_size_inches(w=5, h=4)\n",
        "plt.title('polynomial regression model of degree $3$')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7kq8ePVbZkZ"
      },
      "source": [
        "That's a pretty good fit!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeDOkxrdx-BH"
      },
      "source": [
        "### Quantifying goodness of fit\n",
        "\n",
        "We can also numerically judge the goodness of fit through the [_mean squared error_](https://en.wikipedia.org/wiki/Mean_squared_error) metric, defined as\n",
        "\n",
        "$$\n",
        "MSE(\\mathbf{y},\\hat{\\mathbf{y}})  = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2,\n",
        "$$\n",
        "\n",
        "where $\\hat{y}_i$ has the same meaning here that it does in [the book](https://mml.johnmyersmath.com/stats-book/chapters/12-models.html#linear-regression-models). It is the predicted $y$-value of the $i$-th instance in the dataset:\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\beta_0 + \\beta_1 x_i + \\cdots + \\beta_d x_i^d.\n",
        "$$\n",
        "\n",
        "The next code cell shows you how to compute the MSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJbhQiZ63xdb"
      },
      "outputs": [],
      "source": [
        "# Import the metric from the `metrics` submodule.\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Get the predictions from the model.\n",
        "y_hat = model.predict(X_poly)\n",
        "\n",
        "# Compute the MSE.\n",
        "mse = mean_squared_error(y, y_hat)\n",
        "print(f'The mean squared error is {mse:0.4f}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYzBv8xl4hIB"
      },
      "source": [
        "As the name mean squared **error** suggests, the smaller the MSE, the better.\n",
        "\n",
        "Let's recap the workflow to build and train a polynomial regression model:\n",
        "\n",
        "1. Create a new design matrix `X_poly` from the original design matrix `X`.\n",
        "2. Fit a linear regression model to the design matrix `X_poly` and the original $y$-values in the array `y`.\n",
        "3. For a visual test for goodness of fit: Get the coefficients from the linear regression model, use them to define the polynomial regression function, and plot this function on top of a scatter plot of the data.\n",
        "4. For a numerical test for goodness of fit: Get the predicted $y$-values by calling the `predict` method on the model. Pass the true $y$-values in `y` and the predicted $y$-values in `y_hat` into the `mean_squared_error` metric.\n",
        "\n",
        "Wouldn't it be nice if we could combine steps (1) and (2) into a **single** object, so that we don't have to explicitly create polynomial features by hand? Conveniently, scikit-learn makes this possible through the `pipeline` submodule. Here's how:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diRmr1tiyDNh"
      },
      "outputs": [],
      "source": [
        "# Import `Pipeline` from the `pipeline` submodule.\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Instantiate `PolynomialFeatures` and `LinearRegression` objects.\n",
        "pf = PolynomialFeatures(degree=3, include_bias=False)\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Toss the `PolynomialFeatures` and `LinearRegression` objects into the\n",
        "# pipeline constructor as a list of tuples. The first entry in the tuple\n",
        "# is the name (or key) to the associated component of the pipeline.\n",
        "model3 = Pipeline([('preprocessor', pf), ('linear regressor', lr)])\n",
        "\n",
        "# We can now access the components of the pipeline by key. For example,\n",
        "# the following line grabs the linear regressor from the pipeline.\n",
        "model3['preprocessor']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1M0OXTD6kSI"
      },
      "source": [
        "In this context, the `PolynomialFeatures` object plays the role of a \"data preprocessor.\" This is why it is called `preprocessor` in the pipeline. We saved the model into the variable `model3` to differentiate it from other models of different degrees that we will use later on.\n",
        "\n",
        "We can now fit the pipeline model to the data using the **original** design matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmdp1aaH6jtK"
      },
      "outputs": [],
      "source": [
        "model3.fit(X, y)\n",
        "\n",
        "# Get the predicted y-values from the pipeline model.\n",
        "y_hat_pipeline = model3.predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAJEsrCB6t-M"
      },
      "source": [
        "Let's make sure the predicted $y$-values from the original model (with `X_poly` constructed by hand) match the predicted values from the pipeline model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EW-d9DC6y0H"
      },
      "outputs": [],
      "source": [
        "# Are the predictions equal?\n",
        "np.array_equal(y_hat, y_hat_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1b1vUN_9Kwd"
      },
      "source": [
        "Finally, you can grab the coefficients $\\beta_0$ and $\\boldsymbol{\\beta}$ from the pipeline model by first grabbing the linear regressor component from the pipeline and accessing the `intercept_` and `coef_` attributes just like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IP1hPvz9Xsu"
      },
      "outputs": [],
      "source": [
        "beta_0, beta = model3['linear regressor'].intercept_, model3['linear regressor'].coef_\n",
        "\n",
        "# Print the coefficients.\n",
        "print(f'beta_0 = {beta0:0.4f}\\nbeta_1 = {beta[0]:0.4f}\\nbeta_2 = {beta[1]:0.4f}\\nbeta_3 = {beta[2]:0.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj3bj0Gt7QKz"
      },
      "source": [
        "#### Problem 1 --- Fitting a polynomial regression model\n",
        "\n",
        "You now have everything you need in order to build polynomial regression models on your own through scikit-learn!\n",
        "\n",
        "In this problem, I want you to construct and fit a polynomial regression model of degree $19$ to our dataset of $m=32$ points in the plane. In the next code cell, construct this model as a pipeline model using the template from above. To differentiate this model from the degree-$3$ one above, we will use the variable `model19`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS5OaXDk71cw"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "pf = None         # <-- replace `None` with your own code\n",
        "lr = None         # <-- replace `None` with your own code\n",
        "model19 = None    # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wSbpnhY8Em2"
      },
      "source": [
        "Now, in the next code cell, fit `model19` to the data in the original design matrix `X` and the $y$-values in the array `y`. Then, get the learned coefficients $\\beta_0$ and $\\boldsymbol{\\beta}$, and also the predicted $y$-values for later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSFJeblf8Mo5"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "None                        # <-- replace `None` with your own code\n",
        "beta0_19, beta_19 = None    # <-- replace `None` with your own code\n",
        "y_hat = None                # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN THIS CELL TO CHECK YOUR ANSWERS\n",
        "\n",
        "prob_check(answers=[beta0_19, beta_19, y_hat], prob_num=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95VxYJqZ8xKJ"
      },
      "source": [
        "#### Problem 2 --- Visualizing goodness of fit\n",
        "\n",
        "We now want to plot the degree-$19$ polynomial regression function on top of a scatter plot to visualize goodness of fit. I've implemented the polynomial regression function for you with call signature `f19(x, beta0, beta)`. Run the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2KjLc2_-SPB"
      },
      "outputs": [],
      "source": [
        "def f19(x, beta0, beta):\n",
        "  return beta0 + sum([beta[k] * x ** (k + 1) for k in range(len(beta))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgVI_Xcs_XkB"
      },
      "source": [
        "In the next code cell, visualize the goodness of fit by plotting the polynomial regression function on the scatter plot. (_Hint_: Copy and paste code from above, making the \"obvious\" changes.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUd-b0X2-udS"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQwCbj6iAYDK"
      },
      "source": [
        "#### Problem 3 --- Quantifying goodness of fit\n",
        "\n",
        "The plot you just produced shows the degree-$19$ model fits the data well in at least one sense: The regression function appears to pass directly through multiple data points. Let's quantify the fit, using the MSE. In the next code cell, compute the MSE for the degree-$19$ model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJZHBxouA0iG"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL.\n",
        "\n",
        "mse = None      # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN THIS CELL TO CHECK YOUR ANSWERS\n",
        "\n",
        "prob_check(answers=[mse], prob_num=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAsbAVgaA57c"
      },
      "source": [
        "Assuming you coded everything correctly, the MSE for the degree-$19$ model should be smaller than the MSE for the degree-$3$ model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbZqEx0DCDQ9"
      },
      "source": [
        "### Overfitting and underfitting\n",
        "\n",
        "So, there are at least two ways in which the degree-$19$ model is \"better\" than the degree-$3$ model:\n",
        "\n",
        "1. The degree-$19$ regression function passes **directly through** more data points than the degree-$3$ model.\n",
        "2. The MSE for the degree-$19$ model is smaller than the MSE for the degree-$3$ model.\n",
        "\n",
        "Does this mean that the degree-$19$ model is _truly_ better than the degree-$3$ model?\n",
        "\n",
        "_Nope_.\n",
        "\n",
        "Here's why: Very often, one of the main uses for regression models is to predict the $y$-values of future (i.e., new) data. In this context, the dataset used to fit the model is often called the _training data_. But the degree-$19$ model fits the training data so well that it is actually \"overfitting\" the data, which means essentially that it is learning the random variations in the data. We don't want this!\n",
        "\n",
        "So, the degree-$19$ model is actually _worse_ than the degree-$3$ model, because it doesn't _generalize_ as well to new data. To drive home this point, let's suppose that we've been given new data. Run the following cell to import it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELbpbPzTyT8D"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/data/data-12-2.csv'\n",
        "df = pd.read_csv(url)\n",
        "X_new = df['x'].to_numpy().reshape(-1, 1)\n",
        "y_new = df['y'].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VbQ-iMSGoNX"
      },
      "source": [
        "Now, run the following cell to compare the degree-$3$ and degree-$19$ models on the new data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jokwl8v2D0o9"
      },
      "outputs": [],
      "source": [
        "_, axes = plt.subplots(ncols=2, figsize=(8, 4), sharex=True, sharey=True)\n",
        "regression_functions = [f3, f19]\n",
        "parameters = [(beta0, beta), (beta0_19, beta_19)]\n",
        "models = [model3, model19]\n",
        "degrees = [3, 19]\n",
        "\n",
        "for f, betas, model, degree, axis in zip(regression_functions, parameters, models, degrees, axes):\n",
        "  y_hat = model.predict(X_new)\n",
        "  mse = mean_squared_error(y_new, y_hat)\n",
        "\n",
        "  axis.scatter(X_new, y_new)\n",
        "  axis.plot(grid, f(grid, *betas), color='orange')\n",
        "\n",
        "  axis.set_ylim(-2, 2)\n",
        "  axis.set_xlabel('$x$')\n",
        "  axis.set_title(f'degree {degree}, MSE = {mse:0.4f}')\n",
        "\n",
        "axes[0].set_ylabel('$y$')\n",
        "plt.suptitle('degree-3 and degree-19 models on new data')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5f7jSRNG8-R"
      },
      "source": [
        "Notice that the MSE for the degree-$3$ model on the new data is roughly the same as the MSE on the training data, while the new MSE for the degree-$19$ is _way_ larger than the MSE on the training data. This is what overfitting looks like!\n",
        "\n",
        "You should think of data as consisting of a combination of \"signal\" and \"noise.\" A good model separates the signal from the noise. Models that overfit are learning too much noise.\n",
        "\n",
        "The problem opposite to overfitting is, of course, called _underfitting_. This occurs when the model is so \"rigid\" that it isn't capable of learning the signal. In our example, this would occur for a linear regression model (i.e., a polynomial regression model of degree $1$). To see what underfitting looks like, the next code cell fits a linear regression model to the original data, then plots the regression line over top of the scatter plot of the _new_ data. It also computes the MSE on the new data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_vRlxGAH_eh"
      },
      "outputs": [],
      "source": [
        "model1 = LinearRegression()\n",
        "model1.fit(X, y)\n",
        "beta0_1, beta_1 = model1.intercept_, model1.coef_\n",
        "y_hat = model1.predict(X_new)\n",
        "mse = mean_squared_error(y_new, y_hat)\n",
        "\n",
        "def f1(x, beta0, beta):\n",
        "  return beta0 + beta * x\n",
        "\n",
        "plt.scatter(X_new, y_new)\n",
        "plt.plot(grid, f1(grid, beta0_1, beta_1), color='orange')\n",
        "plt.ylim(-2, 2)\n",
        "plt.xlabel('$x$')\n",
        "plt.ylabel('$y$')\n",
        "plt.gcf().set_size_inches(w=5, h=4)\n",
        "plt.title(f'linear regression model\\nMSE = {mse:0.4f}')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtuMxYFGI7JZ"
      },
      "source": [
        "Indeed, this plot shows that the the regression line is so \"rigid\" that it can't fit the evident pattern in the data. It's underfitting!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBgoj3YwLU7-"
      },
      "source": [
        "### Model validation\n",
        "\n",
        "So, we want to avoid both overfitting and underfitting. With single-variable polynomial regression models, it's pretty easy to check whether a model is under- or overfitting based on a plot of the regression function. But you can imagine in other situations, in many more dimensions, it's not as easy to gauge under- and overfitting. One could just wait until new data arrives to check for these issues, but this isn't always feasible---your client wants the model *now*, not *later*!\n",
        "\n",
        "All this business falls under the heading of _model validation_, which is the process through which the analyst aims to check whether their proposed model generalizes well to new data.\n",
        "\n",
        "One popular way to validate a model is through _cross validation_. Here's how it works:\n",
        "\n",
        "1. The **original** data is split into _training sets_ and _validation sets_.\n",
        "2. The model is fit to the training set.\n",
        "3. The fitted model is then used to make predictions on the validation set.\n",
        "4. Various goodness-of-fit metrics (like MSE) are computed from the predictions on the validation set.\n",
        "5. The validation metrics in (4) are used as a proxy for the model's ability to generalize to new data.\n",
        "6. If needed, steps (1) through (4) are repeated $k$ times, splitting the data into different training and validation sets each time, and the validation metrics over each training/prediction run are averaged to get the final generalizability proxy.\n",
        "\n",
        "For example, here's a diagram depicting *$4$-fold cross validation*:\n",
        "\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/img/cv.svg\" width=\"500\" align=\"center\">\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "First, the data is split into four subsets of equal size. In the first run, the model is trained on $3/4$-ths of the data, and validation metrics (like MSE) are computed from predictions on the remaining $1/4$-th of the data \"held out\" from the training process. Then, in the second run, a different subset of the data is selected as the validation set, and the model is trained on the remaining $3/4$-ths of the data. Validation metrics are computed on the \"held out\" validation set. And so on.\n",
        "\n",
        "At the end of this process, we will have four values for the validation metric. These are often averaged to compute a final number to serve as a proxy for the model's ability to generalize.\n",
        "\n",
        "Fortunately for us, scikit-learn automates this cross validation process! Here's how easy it is---run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xicC6G3PQ2YV"
      },
      "outputs": [],
      "source": [
        "# Import `cross_val_score` from the `model_selection` submodule.\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Run 4-fold cross validation for the degree-3 model. Pay careful attention to the parameters!\n",
        "cv_mse3 = -cross_val_score(model3, X, y, scoring='neg_mean_squared_error', cv=4)\n",
        "\n",
        "# Print out the cross validation MSE's.\n",
        "cv_mse3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCnC-apeSpu2"
      },
      "source": [
        "The code in this cell computes the MSE's over a $4$-fold cross validation (that's the `cv=4` parameter) of our degree-$3$ polynomial regression model. Notice that the code prints out all four of the MSE's. Notice also that the `cross_val_score` function actually returns the **negative** MSE, which accounts for the negative sign after the equal sign `=`.\n",
        "\n",
        "Let's average the four MSE's to get our final proxy for generalizability:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8ecjxbMS1Ij"
      },
      "outputs": [],
      "source": [
        "cv_mse3.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkGE9-znS-si"
      },
      "source": [
        "Now scroll back up and look at the _actual_ MSE on the new data for the degree-$3$ model. It's not _exactly_ equal to the cross validated MSE, but it's close!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp2LT4NYTbF_"
      },
      "source": [
        "#### Problem 4 --- Validating the degree-19 and degree-1 models\n",
        "\n",
        "In the next code cell, compute a $4$-fold cross validated MSE for the degree-$19$ model on the **original data** in the design matrix `X` and the array `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylJ0XeVT5UI"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "cv_mse19 = None         # <-- replace `None` with your own code\n",
        "cv_mse19.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nIj-mjyUEHl"
      },
      "source": [
        "If you coded everything correctly, the cross validated MSE should be _huge_, indicating the degree-$19$ model's ability to generalize is terrible.\n",
        "\n",
        "Now compute the $4$-fold cross validated MSE for the degree-$1$ linear regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfQR9fZrUTCn"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "cv_mse1 = None          # <-- replace `None` with your own code\n",
        "cv_mse1.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN THIS CELL TO CHECK YOUR ANSWERS\n",
        "\n",
        "prob_check(answers=[cv_mse19, cv_mse1], prob_num=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqTbk2LLUj0l"
      },
      "source": [
        "As far as cross validated MSE's go, this analysis conclusively proves that the degree-$3$ model is better than the other two. The degree-$19$ model suffers from extreme overfitting, while the degree-$1$ model underfits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1wrE2p0kQad"
      },
      "source": [
        "## Naive Bayes models\n",
        "\n",
        "The scikit-learn libary is so well-designed that once you understand the workflow for one model in the library, you pretty much understand them all. The purpose of the rest of this assignment is to prove this point by building a spam classifier via a [_Naive Bayes model_](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcFvEcV7WJ4I"
      },
      "source": [
        "### A spam classifier\n",
        "\n",
        "To begin, let's suppose that we have a collection of emails. We let $Y \\sim Ber(\\psi)$ be an indicator random variable ($\\psi$ is a parameter---see below) with $Y=1$ corresponding to a spam email, and $Y=0$ indicating a non-spam email. The goal is to predict the value of $Y$ based on the presence of certain words in the email.\n",
        "\n",
        "Let's suppose that we are on the lookout for only six words:\n",
        "\n",
        "$$\n",
        "\\text{office, cash, vacation, meeting, credit, cat}.\n",
        "$$\n",
        "\n",
        "It is natural to define a $6$-dimensional random vector\n",
        "\n",
        "$$\n",
        "\\mathbf{X}^\\intercal = (X_1,X_2,X_3,X_4,X_5,X_6)\n",
        "$$\n",
        "\n",
        "where each component $X_j$ is an indicator random variable for the presence of these words, written in that order. So, for example, a value of $X_3=1$ means that a given email contains the word \"vacation.\" We will try to predict the value of $Y$ based on the value of $\\mathbf{X}$.\n",
        "\n",
        "To do this, we will use a probabilistic graphical model of the form\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/img/nb.svg\" width=\"200\" align=\"center\">\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "where the parameters are given by\n",
        "\n",
        "* a real number $\\psi \\in [0,1]$,\n",
        "* two $6$-dimensional vectors $\\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_1 \\in [0,1]^6$.\n",
        "\n",
        "The real number $\\psi$ parametrizes the distribution of $Y\\sim Ber(\\psi)$, while the link function at $\\mathbf{X}$ is given by\n",
        "\n",
        "$$\n",
        "p(\\mathbf{x} \\mid y; \\ \\boldsymbol{\\theta}_0,\\boldsymbol{\\theta}_0) = \\prod_{j=1}^6 \\phi_j^{x_j} (1-\\phi_j)^{1-x_j} \\quad \\text{where} \\quad \\boldsymbol{\\phi} = (1-y)\\boldsymbol{\\theta}_0 + y \\boldsymbol{\\theta}_1\n",
        "$$\n",
        "\n",
        "and $\\boldsymbol{\\phi}^\\intercal = (\\phi_1,\\phi_2,\\ldots,\\phi_6)$. Thus, the components $X_j$ of the random vector $\\mathbf{X}$ are conditionally independent (given $Y$) Bernoulli random variables, with\n",
        "\n",
        "$$\n",
        "X_j \\mid Y \\sim Ber(\\phi_j), \\quad j=1,2,\\ldots,6.\n",
        "$$\n",
        "\n",
        "If we write\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\theta}_i^\\intercal = (\\theta_{i1},\\theta_{i2},\\ldots, \\theta_{i6})\n",
        "$$\n",
        "\n",
        "for each $i=0,1$, then $\\theta_{0j}$ is the probability that the $j$-th word appears in the email, given that it is _not_ a spam email (i.e., $y=0$), while $\\theta_{1j}$ is the probability that it appears, given that it _is_ a spam email (i.e., $y=1$). This is an example of a _Naive Bayes model_. The first part of the name, \"Naive,\" comes from the assumption of conditional independence of the word indicator random variables $X_j$. This is a \"naive\" assumption, because it is clearly false in the real world.\n",
        "\n",
        "Once all these parameters have been learned from training data, we may predict whether a given email is spam based on the presence of the six words by examining the two conditional probabilities\n",
        "\n",
        "$$\n",
        "p(y =0 \\mid \\mathbf{x}) \\quad \\text{and} \\quad p(y=1 \\mid \\mathbf{x}),\n",
        "$$\n",
        "\n",
        "\n",
        "where we've dropped the parameters from the notation for simplicity. If the first probability is larger than the second, we predict non-spam (i.e., $y=0$); otherwise, we predict spam (i.e., $y=1$). In symbols, if we write $\\hat{y}$ for our prediction of $y$, we have\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\argmax_{y\\in \\{0,1\\}} p(y \\mid \\mathbf{x}),\n",
        "$$\n",
        "\n",
        "where the \"$\\argmax$\" operator returns the maximizer of the function to the right.\n",
        "\n",
        "These conditional probabilities may be computed using (you guessed it) Bayes' theorem:\n",
        "\n",
        "$$\n",
        "p(y \\mid \\mathbf{x}) = \\frac{p(y) p(\\mathbf{x} \\mid y)}{p(\\mathbf{x})}.\n",
        "$$\n",
        "\n",
        "But if all we are after is to determine which of the two probabilities above is larger, than we may drop the probability $p(\\mathbf{x})$ from Bayes' theorem (since it does not depend on $y$) and write\n",
        "\n",
        "$$\n",
        "p(y \\mid \\mathbf{x}) \\propto p(y) p(\\mathbf{x} \\mid y) = p(\\mathbf{x},y)\n",
        "$$\n",
        "\n",
        "Then, our prediction is given by\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\argmax_{y\\in \\{0,1\\}} \\left[p(y) p(\\mathbf{x} \\mid y)\\right] = \\argmax_{y\\in \\{0,1\\}} \\left[p(\\mathbf{x}, y)\\right]\n",
        "$$\n",
        "\n",
        "If this seems confusing, don't worry too much right now, because luckily scikit-learn takes care of a lot of the details for us. To see how, let's import some data on emails:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGq2Y8sIZXbR"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/jmyers7/stats-book-materials/main/data/data-12-3.csv'\n",
        "df = pd.read_csv(url)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDCuXS76h_na"
      },
      "source": [
        "From the printout, we see that we have data on 512 emails. The first row in the dataframe shows that the first email is spam, and it contains the words \"office,\" \"cash,\" \"vacation\", and \"credit\".\n",
        "\n",
        "Let's pull out the $x_j$'s and the $y$'s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q5ir4hUZgRn"
      },
      "outputs": [],
      "source": [
        "X = df[['x1', 'x2', 'x3', 'x4', 'x5', 'x6']].to_numpy()\n",
        "y = df['y'].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CKoW5J9ia9s"
      },
      "source": [
        "#### Problem 5 --- Fitting our spam classifier\n",
        "\n",
        "In the next code cell, you will implement our spam classifier as an object from [`BernoulliNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) in scikit-learn. In it, instantiate the model with the default parameters passed into `BernoulliNB`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIUIiyxcZsGI"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "model = None      # <-- replace `None` with your own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozdEzPiDjjZW"
      },
      "source": [
        "#### Problem 6 --- Validating the spam classifier\n",
        "\n",
        "Let's validate our spam classifier, to see if it generalizes well to new emails. In the next code cell, compute $6$-fold cross validated accuracy scores for the model. (_Hint:_ We talked about the _accuracy_ metric in the worksheet for this chapter. You'll need to set the `scoring` parameter to `'accuracy'`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnkjOtekjaKj"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "cv_accuracy = None        # <-- replace `None` with your own code\n",
        "cv_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkzsGhhtpKcS"
      },
      "source": [
        "Assuming your code is correct, you should see a NumPy array with the six accuracy scores for each of the six training/validation runs. In the next code cell, take the average of these scores to get the final proxy for the model's generalizability:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R0pfd0ajNyU"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR CODE IN THIS CELL\n",
        "\n",
        "accuracy_mean = None        # <-- replace `None` with your own code\n",
        "accuracy_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RUN THIS CELL TO CHECK YOUR ANSWERS\n",
        "\n",
        "prob_check(answers=[cv_accuracy, accuracy_mean], prob_num=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jf27xH-pVc6"
      },
      "source": [
        "If your code is correct, you should see a number pretty close 1. This means that our spam classifier should generalize well to new emails!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
